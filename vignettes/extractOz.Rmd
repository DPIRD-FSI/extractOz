---
title: "extractOz"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{extractOz}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding
  %\VignetteDepends{tidyverse}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Load libraries

```{r setup, message=FALSE}
library(extractOz)
library(tidyverse)
```

## Create locations in WA and NSW

```{r create-location}
# create a list of locations for use in testing
locs <- list(
  "Merredin" = c(x = 118.28, y = -31.48),
  "Corrigin" = c(x = 117.87, y = -32.33),
  "Tamworth" = c(x = 150.84, y = -31.07)
)
```

## Extract the GRDC AgroEcological Zones

See `?extract_ae_zone()` for more help on how to use this function.

```{r extract-zones, message=FALSE, eval=FALSE}
z <- extract_ae_zone(x = locs)
```

## Extract the soil order

See `?extract_daas_soil_order()` for more help on how to use this function.

```{r extract-soil, eval=FALSE}
s <- extract_daas_soil_order(x = locs)
```

## Get Weather Data for these Locations in 2020

Using the previously created list of GPS points, fetch station observation weather data from SILO for 2020.
This is a non-working example, replace `your_api_key` with your email address below.
See `?extract_patched_point()` for more help on how to use this function.

```{r get-weather, eval=FALSE}
three_sites  <-
  extract_patched_point(
    x = locs,
    start_date = "20200101",
    end_date = "20201231",
    api_key = "adamhsparks@gmail.com"
  )
```

## Join the Weather Data with AE Zone, Soil Order and Site Information

Now using `dplyr::left_join()`, create a single `data.frame()` of the location, GPS coordinates, agroecological zone and weather data.

```{r join, eval=FALSE}
left_join(z, three_sites, by = c("location" = "location")) %>%
  left_join(s)
```

## Creating a Pipeline

You can use {magrittr's} pipe, `%>%`, to string the whole set of commands together into a pipeline to ingest, extract information and download weather data for your GPS points.
Here, we will import data from a CSV file, extract the values, fetch the weather data and create a `data.frame` object with all of the AE Zone, soil order, weather and site information.

The CSV file is included in this package so that you can run this example yourself by providing your own e-mail address for the SILO API key.

Using the helper function, `df_to_list()` in the pipeline converts the [tibble::tibble()] from [readr::read_csv()] into a `list()` object that can be used in any of the functions found in {extractOz}.

The `left_join()` takes the existing `data.frame` that has the location information, AE Zone and soil order as `x = .`.
The `y` for the left join is then the `get_patched_point()` and details are provided to that API, again the original `data.frame` that has the `x` and `y` information and `Sitename` are provided as `.$colname`.

The final step is to save the `data.frame` as an object, `complete_data`.

```{r pipeline, eval=FALSE}
x <- read_csv(system.file(
  "extdata",
  "sample_points.csv",
  package = "extractOz",
  mustWork = TRUE
))

# create a data.frame of the soil order and AE Zone
x %>%
  df_to_list() %>%
  left_join(
    x = extract_ae_zone(x = .),
    y = extract_daas_soil_order(x = .),
    by = c("x", "y", "location")
  ) -> soil_zone

# join the `soil_zone` with weather data from SILO
  left_join(
    x = soil_zone,
    y = extract_patched_point(
      x = df_to_list(x),
      start_date = "20200101",
      end_date = "20201231",
      api_key = "your_api_key"
    ),
    by = c("location" = "location")
  ) -> complete_data
```
